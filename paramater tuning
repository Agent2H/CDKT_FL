-------------Round number:  7  -------------
============= Test Client Models - Specialization ============= 
At round 7 AvgC. testing accuracy: 0.9640479360852197
At round 7 AvgC. training accuracy: 0.9920607525025888
============= Test Client Models - Generalization ============= 
At round 7 AvgC. testing accuracy: 0.18798934753661783
============= Test Global Models  ============= 
At round 7 global testing accuracy: 0.38881491344873503


tensor([3, 7, 8, 1, 5, 9, 4, 8, 5, 0])

Full model, alpha=beta=2, loop all batch public
-------------Round number:  29  -------------
============= Test Client Models - Specialization ============= 
At round 29 AvgC. testing accuracy: 0.9819277108433735
At round 29 AvgC. training accuracy: 1.0
============= Test Client Models - Generalization ============= 
At round 29 AvgC. testing accuracy: 0.8240963855421687
============= Test Global Models  ============= 
At round 29 global testing accuracy: 0.891566265060241


Full model, alpha=beta=2, no loop all batch public
-------------Round number:  29  -------------
============= Test Client Models - Specialization ============= 
At round 29 AvgC. testing accuracy: 0.9578313253012049
At round 29 AvgC. training accuracy: 1.0
============= Test Client Models - Generalization ============= 
At round 29 AvgC. testing accuracy: 0.7710843373493976
============= Test Global Models  ============= 
At round 29 global testing accuracy: 0.927710843373494


Full model = false, public = 700
-------------Round number:  29  -------------
============= Test Client Models - Specialization ============= 
At round 29 AvgC. testing accuracy: 0.9819277108433735
At round 29 AvgC. training accuracy: 1.0
============= Test Client Models - Generalization ============= 
At round 29 AvgC. testing accuracy: 0.21987951807228914
============= Test Global Models  ============= 
At round 29 global testing accuracy: 0.9216867469879518

Full model = False, beta =0, alpha = 2 public = 355
At round 29 AvgC. testing accuracy: 0.9820359281437125
At round 29 AvgC. training accuracy: 1.0
============= Test Client Models - Generalization ============= 
At round 29 AvgC. testing accuracy: 0.21976047904191617
============= Test Global Models  ============= 
At round 29 global testing accuracy: 0.7964071856287425


alpha=beta=0
-------------Round number:  29  -------------
============= Test Client Models - Specialization ============= 
At round 29 AvgC. testing accuracy: 0.9820359281437125
At round 29 AvgC. training accuracy: 1.0
============= Test Client Models - Generalization ============= 
At round 29 AvgC. testing accuracy: 0.21976047904191617
============= Test Global Models  ============= 
At round 29 global testing accuracy: 0.7964071856287425


FedAvg:global acc = 88%


1/fmnist, full model = false, metric global = metric local = KL
alpha: 0.1->0.5
decrease alpha: spec up, gen down, global acc stable.

best performance : alpha =0.4 beta=0.6 gen=80, global = 84, spec = 95.3 global_lr = 0.04, local_lr =0.03
2/fmnist, full model = false, metric norm 2 
global metric = KL, local metric = norm 2: alpha=0.2, beta=0.03, global lr=local lr=0.01 increase global distillation epoch => increase gen and global per but decrease spec

Setting: get subset of users
Same model: alpha =5, beta =1, local lr =0.015, global lr=0.02
Heterogeneous model: alpha =2, beta=1, local lr = 0.015, global lr= 0.02



